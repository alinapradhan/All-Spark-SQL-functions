# Spark SQL All Functions Pyspark

A comprehensive reference guide to using Spark SQL functions in PySpark. This notebook demonstrates how to apply a wide range of built-in SQL functions for data processing, transformation, and analysis in Apache Spark.
 
## Features:
* Covers over 100+ commonly used Spark SQL functions
* Categorized by function types (string, date, aggregation, window, etc.)
* Real-world usage examples with DataFrame inputs 
* Syntax and behavior illustrations for each function
* Easy to run and modify for quick testing

## Requirements:
* Python 3.x
* Apache Spark (>= 3.0)
* PySpark library
* Jupyter Notebook or compatible IDE

## How to Use
1. Clone the repository or download the notebook:

   ```bash
   git clone <repo_url>
   ```
2. Launch Jupyter and open `SPARKSQLAllFunctionsPyspark.ipynb`
3. Follow the examples and run code cells to explore each function.

## Structure

Each section focuses on a specific group of SQL functions, such as:

* String functions (`concat`, `substr`, `upper`, etc.)
* Date/time functions (`current_date`, `datediff`, etc.)
* Aggregate functions (`sum`, `avg`, `count`, etc.)
* Window functions (`rank`, `dense_rank`, etc.)
* Conditional functions (`when`, `coalesce`, etc.)


## Recommended For

* Data Engineers
* Spark Developers
* Data Scientists working on scalable data pipelines
* Anyone preparing for PySpark technical interviews



